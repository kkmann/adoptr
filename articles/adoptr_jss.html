<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The adoptr Package: Adaptive Optimal Designs for Clinical Trials in R • adoptr</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.3/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="The adoptr Package: Adaptive Optimal Designs for Clinical Trials in R">
<meta property="og:description" content="adoptr">
<meta property="og:image" content="https://kkmann.github.io/adoptr/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">adoptr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.4.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/adoptr.html">Get started</a>
</li>
<li>
  <a href="../articles/adoptr_jss.html">Overview Paper</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/working-with-priors.html">Working with Priors</a>
    </li>
    <li>
      <a href="../articles/conditional-scores.html">Conditional Scores</a>
    </li>
    <li>
      <a href="../articles/composite-scores.html">Composite Scores</a>
    </li>
    <li>
      <a href="../articles/defining-new-scores.html">Definining New Scores</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/kkmann/adoptr">
    <span class="fas fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="adoptr_jss_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>The adoptr Package: Adaptive Optimal Designs for Clinical Trials in R</h1>
                        <h4 class="author">Kevin Kunzmann</h4>
            <address class="author_afil">
      Cambridge University<br><a class="author_email" href="mailto:#"></a><a href="mailto:kevin.kunzmann@mrc-bsu.cam.ac.uk" class="email">kevin.kunzmann@mrc-bsu.cam.ac.uk</a>
      </address>
                              <h4 class="author">Maximilian Pilz</h4>
            <address class="author_afil">
      University of Heidelberg<br><a class="author_email" href="mailto:#"></a><a href="mailto:pilz@imbi.uni-heidelberg.de" class="email">pilz@imbi.uni-heidelberg.de</a>
      </address>
                              <h4 class="author">Carolin Herrmann</h4>
            <address class="author_afil">
      Charité Berlin and Berlin Insitute of Health<br><a class="author_email" href="mailto:#"></a><a href="mailto:carolin.herrmann@charite.de" class="email">carolin.herrmann@charite.de</a>
      </address>
                              <h4 class="author">Geraldine Rauch</h4>
            <address class="author_afil">
      Charité Berlin and Berlin Insitute of Health<br><a class="author_email" href="mailto:#"></a><a href="mailto:geraldine.rauch@charite.de" class="email">geraldine.rauch@charite.de</a>
      </address>
                              <h4 class="author">Meinhard Kieser</h4>
            <address class="author_afil">
      University of Heidelberg<br><a class="author_email" href="mailto:#"></a><a href="mailto:meinhard.kieser@imbi.uni-heidelberg.de" class="email">meinhard.kieser@imbi.uni-heidelberg.de</a>
      </address>
                  
            <h4 class="date">2021-05-28</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/kkmann/adoptr/blob/master/vignettes/adoptr_jss.Rmd"><code>vignettes/adoptr_jss.Rmd</code></a></small>
      <div class="hidden name"><code>adoptr_jss.Rmd</code></div>

    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      <p>Even though adaptive two-stage designs with unblinded interim analyses are becoming increasingly popular in clinical trial designs, there is a lack of statistical software to make their application more straightforward. The package <strong>adoptr</strong> fills this gap for the common case of two-stage one- or two-arm trials with (approximately) normally distributed outcomes. In contrast to previous approaches, <strong>adoptr</strong> optimizes the entire design upfront which allows maximal efficiency. To facilitate experimentation with different objective functions, <strong>adoptr</strong> supports a flexible way of specifying both (composite) objective scores and (conditional) constraints by the user. Special emphasis was put on providing measures to aid practitioners with the validation process of the package.</p>
    </div>
    
<p>This manuscript was accepted for publication in the Journal of Statistical Software.</p>
<div id="background" class="section level1">
<h1 class="hasAnchor">
<a href="#background" class="anchor"></a>Background</h1>
<p>Confirmatory clinical trials are conducted in a strictly regulated environment. A key quality criterion put forward by the relevant agencies (<span class="citation">US Food and Drug Administration and others (2019)</span>, <span class="citation">Committee for Medicinal Products for Human Use and others (2007)</span>) for a study that is supposed to provide evidence for the regulatory acceptance of a new drug or treatment is strict type one error rate control. This requirement was often seen as conflicting with the perceived need to make trials more flexible by, e.g., early stopping for futility, group-sequential enrollment, or even adaptive sample size recalculation. An excellent historical review of the development of the field of adaptive clinical trial designs and the struggles along the way is given in <span class="citation">Bauer et al. (2015)</span>.</p>
<p>In this manuscript, the focus lies exclusively on adaptive two-stage designs with one unblinded interim analysis. Both early stopping for futility and efficacy are allowed and the final sample size as well as the critical value to reject the null hypothesis is chosen in a data-driven way. There is a plethora of methods for modifying the design of an ongoing trial based on interim results without compromising type one error rate control <span class="citation">(Bauer et al. 2015)</span> but the criteria for deciding <em>which</em> adaptation should be performed during an interim analysis and <em>when</em> to perform the interim analysis are still widely based on heuristics. Bauer <em>et al.</em> mention this issue of guiding adaptive decisions at interim in a principled (i.e., ‘optimal’) way by stating that ‘[t]he question might arise if potential decisions made at interim stages might not be better placed to the upfront planning stage.’ Following <span class="citation">Mehta and Pocock (2011)</span>, <span class="citation">Jennison and Turnbull (2015)</span> developed a principle approach to optimal interim sample size modifications, i.e., to conduct the interim decision (conditional on interim results) such that it optimizes an unconditional performance score. Their approach, however, was still restricted unnecessarily. Recently, <span class="citation">Pilz et al. (2019)</span> extended the work to a fully general variational problem where the optimization problem for any given performance score (optionally under further constraints) is solved over both the sample size adaptation function and the critical value function and the time point of the interim decision simultaneously. This approach is an application of ideas which have been put forward in single-arm trials with binary endpoint for several years (<span class="citation">Englert and Kieser (2013)</span>, <span class="citation">Kunzmann and Kieser (2016)</span>, <span class="citation">Kunzmann and Kieser (2020)</span>) to a setting with continuous test statistics. Clearly, by relaxing the problem to continuous sample sizes and test statistics, the theory becomes much more tractable, and important connections between conditional and unconditional optimality can be discussed much easier <span class="citation">(Pilz et al. 2019)</span>.</p>
<p>A key insight from this recent development is the fact that the true challenge in designing an adaptive trial is less the technical methodology for controlling the type one error rate but rather the choice of the optimality criterion. This issue is much less pressing in single-stage designs since most sensible criteria will be equivalent to minimizing the overall sample size. Thus, in this case, a ‘design’ is often completely specified by given power and type one error rate constraints. For the more complex adaptive designs, however, there are much more sensible criteria (minimize maximal sample size, expected sample size, expected costs, etc.) and the balance between conditional and unconditional properties must be explicitly specified (cf. Section <a href="#sec:Examples">6</a>). This added complexity might be seen as daunting by practitioners, but it is also a chance for tailoring adaptive designs more specifically to a particular situation. The <strong>R</strong>-package <span class="citation">(R Core Team 2019)</span> <strong>adoptr</strong> aims at providing a simple yet customizable interface to specifying a broad class of objective functions and constraints for single- or two-arm, one- or two-stage designs with approximately normally distributed endpoints. The goal of <strong>adoptr</strong> is to enable relatively easy experimentation with different notions of optimality to shift the focus from <em>how</em> to optimize to <em>what</em> to optimize.</p>
<p>In the following, we first give a definition of the problem setting addressed in <strong>adoptr</strong> and the technicalities of translating the underlying variational problem to a simple multivariate optimization problem before motivating the need for an <strong>R</strong>-package. We then present the core functionality of <strong>adoptr</strong> before addressing the issue of facilitating validation of open-source software in a regulated environment and discussing potential future work on <strong>adoptr</strong>.</p>

</div>
<div id="sec:setting" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:setting" class="anchor"></a>Setting</h1>
<p>We consider the problem of a two-stage, two-arm design to establish superiority of treatment over placebo with respect to the mean difference. Assume that to that end data <span class="math inline">\(Y_j^{g, i}\)</span> is observed for the <span class="math inline">\(j\)</span>-th individual of the trial in stage <span class="math inline">\(i\in\{1,2\}\)</span> under treatment (<span class="math inline">\(g=T\)</span>) or placebo (<span class="math inline">\(g=C\)</span>). Let <span class="math inline">\(n_i\)</span> be the per-group sample size in stage <span class="math inline">\(i\)</span> and consider the stage-wise test statistics <span class="math display">\[
  X_i := \frac{\sum_{j=1}^{n_i}Y_j^{T,i} - \sum_{j=1}^{n_i}Y_j^{C,i}}{\sigma\,\sqrt{2\,n_i}}
\]</span> for <span class="math inline">\(i=1,2\)</span>. Under the assumption that the <span class="math inline">\(Y_j^{g, i}\stackrel{iid}{\sim} F_g\)</span> with <span class="math inline">\(\boldsymbol{E}[F_T] - \boldsymbol{E}[F_C] = \theta\)</span> and common variance <span class="math inline">\(\sigma^2\)</span>, by the central limit theorem, the asymptotic distribution of <span class="math inline">\(X_1\)</span> is <span class="math inline">\(\mathcal{N}(\sqrt{n_1/2}\, \theta, 1)\)</span>. Formally, the null hypothesis for the superiority test is thus <span class="math inline">\(\mathcal{H}_0:\theta\leq 0\)</span>. Based on the interim outcome <span class="math inline">\(X_1\)</span>, a decision can be made whether to either stop the trial early for futility if <span class="math inline">\(X_1&lt;c_1^f\)</span>, to stop the trial early for efficacy (early rejection of the null hypothesis) if <span class="math inline">\(X_1&gt;c_1^e\)</span>, or to enter stage two if <span class="math inline">\(X_1\in[c_1^f, c_1^e]\)</span>. Conditional on proceeding to a second stage, it holds that <span class="math inline">\(X_2\,|\,X_1\in[c_1^f, c_1^e]\sim\mathcal{N}(\sqrt{n_2/2}\, \theta, 1)\)</span>. In the second stage, the null hypothesis is rejected if and only if <span class="math inline">\(X_2 &gt; c_2(X_1)\)</span> for a stage-two critical value  <span class="math inline">\(c_2:x_1\mapsto c_2(x_1)\)</span>. To test <span class="math inline">\(\mathcal{H}_0\)</span> at a significance level of <span class="math inline">\(\alpha\)</span>, the stage-one critical values <span class="math inline">\(c_1^f\)</span> and <span class="math inline">\(c_1^e\)</span> as well as <span class="math inline">\(c_2(\cdot)\)</span> must be chosen in a way that protects the overall maximal type one error rate <span class="math inline">\(\alpha\)</span>. Note that it is convenient to define <span class="math inline">\(c_2(x_1) = \infty\)</span> if <span class="math inline">\(x_1&lt;c_1^f\)</span> and <span class="math inline">\(c_2(x_1) = -\infty\)</span> if <span class="math inline">\(x_1&gt;c_1^e\)</span> since the power curve of the design is then given by <span class="math inline">\(\theta\mapsto\boldsymbol{Pr}_\theta\big[X_2&gt;c_2(X_1)\big]\)</span>. This results in a classical group-sequential design and several methods were proposed in the literature for choosing the early-stopping boundaries <span class="math inline">\(c_1^f\)</span> and <span class="math inline">\(c_1^e\)</span> (<span class="citation">O’Brien and Fleming (1979)</span>, <span class="citation">Pocock (1977)</span>) and for defining the stage-two rejection boundary function <span class="math inline">\(c_2(\cdot)\)</span> (<span class="citation">Bauer and Köhne (1994)</span>, <span class="citation">Hedges and Olkin (1985)</span>). Often, the inverse-normal combination test <span class="citation">(Lehmacher and Wassmer 1999)</span> is applied and <span class="math inline">\(c_2(\cdot)\)</span> is defined as a linear function of the stage-one test-statistic <span class="math display">\[ 
  c_2(x_1) = \frac{c - w_1 x_1}{w_2}‚
\]</span> for a critical value <span class="math inline">\(c\)</span> and predefined weights <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>. Most commonly, the stage-wise test statistics are weighted in terms of their respective sample sizes, i.e., <span class="math inline">\(w_1 = \sqrt{n_1 / (n_1+n_2)}\)</span> and <span class="math inline">\(w_2 = \sqrt{n_2 / (n_1+n_2)}\)</span>. This choice of the weights is optimal in the sense that it minimizes the variance of the final test statistic if the assumed sample sizes are indeed realized <span class="citation">(Zaykin 2011)</span>. Note, however, that such prespecified weights become inefficient if the sample size deviates strongly from the anticipated value (cf. <span class="citation">Wassmer and Brannath (2016)</span>, chapter 6.2.5). A natural extension of this group-sequential framework is to allow the second stage sample size to also depend on the observed interim outcome, i.e., to consider a function <span class="math inline">\(n_2:x_1\mapsto n_2(x_1)\)</span> instead of a fixed value <span class="math inline">\(n_2\)</span>. Such ‘adaptive’ two-stage designs are thus completely characterized by a five-tuple <span class="math inline">\(\mathcal{D}:=\big(n_1, c_1^f, c_1^e, n_2(\cdot), c_2(\cdot)\big)\)</span>.</p>
<p>While the required sample size and the critical value for a single-stage design are uniquely defined by given type one error rate and power constraints, it is much less clear how the design parameters of a two-stage design should be selected. This is especially true since both <span class="math inline">\(n_2\)</span> and <span class="math inline">\(c_2\)</span> are functions and thus the parameter space is in fact infinite-dimensional. In order to compare different choices of the design parameters, appropriate scoring criteria are essential. A widely applied criterion is the expected sample size under the alternative hypothesis (see, e.g., <span class="citation">Jennison and Turnbull (2015)</span>). However, there is a variety of further scoring criteria that could be incorporated or even combined in order to rate a two-stage design. For instance, conditional power is defined as the probability to reject the null hypothesis under the alternative given the interim result <span class="math inline">\(X_1=x_1\)</span>: <span class="math display">\[
\operatorname{CP}_\theta(x_1) := \boldsymbol{Pr}_\theta \big[X_2 &gt; c_2(X_1) \, \big| \, X_1=x_1\big].
\]</span> Hence, conditional power is a conditional score given as the power conditioned on the first-stage outcome <span class="math inline">\(X_1=x_1\)</span>. <em>Vice versa</em>, power can be seen as an unconditional score that is obtained by integrating conditional power over all possible stage-one outcomes, i.e., <span class="math display">\[
\operatorname{Power_\theta} = \boldsymbol{E}_\theta\big[\operatorname{CP}(X_1)\big].
\]</span> Intuitively, it makes sense to require a minimal conditional power upon continuation to the second stage since one might otherwise continue a trial with little prospect of still rejecting the null hypothesis. We demonstrate the consequences of this heuristic in Section <a href="#sec:CP_constraint">6.4</a>. Once the scoring criterion is selected, the design parameters may be chosen in order to optimize this objective. The first ones to address this problem were <span class="citation">Jennison and Turnbull (2015)</span> who minimized the expected sample size <span class="math display">\[
\text{ESS}_{\theta}(\mathcal{D}) := \boldsymbol{E}_{\theta} \big[n(X_1) \big] :=
\boldsymbol{E}_{\theta} \big[n_1 + n_2(X_1) \big]
\]</span> of a two-stage design for given <span class="math inline">\(n_1,c_1^f, c_1^e\)</span> with respect to <span class="math inline">\(n_2(\cdot)\)</span> for given power and type one error rate constraints. The function <span class="math inline">\(c_2(\cdot)\)</span>, however, was not optimized. Instead, <span class="citation">Jennison and Turnbull (2015)</span> used a combination test approach to derive <span class="math inline">\(c_2\)</span> given <span class="math inline">\(n_2(\cdot)\)</span> and <span class="math inline">\(n_1\)</span> (cf. Equation @ref(eq:inverse-normal)). In <span class="citation">Pilz et al. (2019)</span>, the authors demonstrated that this restriction is not necessary and that the variational problem of deriving both functions <span class="math inline">\(n_2(\cdot)\)</span> and <span class="math inline">\(c_2(\cdot)\)</span> given <span class="math inline">\(n_1,c_1^f, c_1^e\)</span> to minimize expected sample size can be solved by analyzing the corresponding Euler-Lagrange equation. Nesting this step in a standard optimization over the stage-one parameters allows identifying an optimal set of all design parameters without imposing parametric assumptions on <span class="math inline">\(c_2(\cdot)\)</span>. As a result, a fully optimal design <span class="math inline">\(\mathcal{D}^*:=\big(n_1^*, c_1^{f,*}, c_1^{e, *}, n_2^*(\cdot), c_2^*(\cdot)\big)\)</span> for the following general optimization problem was derived <span class="math display">\[\begin{align}
&amp; \text{minimize} &amp;&amp; \operatorname{ESS}_{\theta_1}(\mathcal{D}) &amp;&amp;&amp;&amp; \\
&amp; \text{subject to:}
&amp;&amp; \boldsymbol{Pr}_{\theta_0}\big[X_2&gt;c_2(X_1)\big] &amp;&amp;\leq \alpha, &amp;&amp;&amp;&amp; \\
&amp;&amp;&amp; \boldsymbol{Pr}_{\theta_1}\big[X_2&gt;c_2(X_1)\big] &amp;&amp;\geq 1-\beta &amp;&amp;&amp;&amp;
\end{align}\]</span> where <span class="math inline">\(\theta_0=0\)</span>.</p>
</div>
<div id="direct-variational-perspective" class="section level1">
<h1 class="hasAnchor">
<a href="#direct-variational-perspective" class="anchor"></a>Direct variational perspective</h1>
<p>In <strong>adoptr</strong>, a simpler solution strategy than solving the Euler-Lagrange equation locally is applied to the same problem class. We propose to embed the entire problem in a finite-dimensional parameter space and solve the corresponding problem over both stage-one and stage-two design parameters simultaneously using standard numerical libraries. I.e., we adopt a <em>direct</em> approach to solving the variational problem. This is done by defining a discrete set of pivot points <span class="math inline">\(\widetilde{x}_1^{(i)}\in(c_1^f, c_1^e), i=1,\ldots,k\)</span>, and interpolating <span class="math inline">\(c_2\)</span> and <span class="math inline">\(n_2\)</span> between these pivots. We use cubic Hermite splines <span class="citation">(Fritsch and Carlson 1980)</span> which are sufficiently flexible, even for a moderate number of pivots, to approximate any realistic stage-two sample size and critical value functions. Since the optimal functions are generally very smooth <span class="citation">(Pilz et al. 2019)</span> they are well suited to spline interpolation. Within the <strong>adoptr</strong> validation report (cf. Section 7) we investigate empirically the shape of the approximated functions and that increasing the number of pivots above a value of 5 to 7 does not improve the optimization results. The latter implies that a relatively small number of pivot points appears to be sufficient to obtain valid spline approximations of the optimal functions. Note that the pivots are only needed in the continuation region since both functions are (piecewise) constant within the early stopping regions. In <strong>adoptr</strong>, the pivots are defined as nodes of a Gaussian quadrature rule of degree <span class="math inline">\(k\)</span>. This choice allows fast and precise numerical integration of any conditional score over the continuation region, e.g., <span class="math display">\[\begin{align} 
\text{ESS}_{\theta}(\mathcal{D})  =
\int n(x_1) f_{\theta} (x_1) \operatorname{d} x_1
\approx n_1 + \sum_{i=1}^{k} \omega_i\, n_2\big(\widetilde{x}_1^{(i)}\big) f_{\theta}
\big(\widetilde{x}_1^{(i)}\big),
\end{align}\]</span> where <span class="math inline">\(f_\theta\)</span> is the probability density function of <span class="math inline">\(X_1\,|\,\theta\)</span> and <span class="math inline">\(\omega_i\)</span> are the corresponding weights of the integration rule. The weights only depend on <span class="math inline">\(k\)</span> and the nodes just need to be scaled to the integration interval. Consequentially, this objective function is smooth in the optimization parameters and the resulting optimization problem is of dimension <span class="math inline">\(2k+3\)</span>, where the tuning parameters are <span class="math inline">\(\big(n_1, c_1^f, c_1^e, n_2\big(\widetilde{x}_1^{(1)}\big), \dots, n_2\big(\widetilde{x}_1^{(k)}\big), c_2\big(\widetilde{x}_1^{(1)}\big), \dots, c_2\big(\widetilde{x}_1^{(k)}\big)\big)\)</span>. Standard numerical solvers may then be employed to minimize it. Since <strong>adoptr</strong> enables generic objectives (cf. Section <a href="#sec:utility_maximization">6.3</a>), it uses the gradient-free optimizer COBYLA <span class="citation">(Powell 1994)</span> internally via the <strong>R</strong>-package <strong>nloptr</strong> (<span class="citation">Johnson (2018)</span>, <span class="citation">Ypma, Borchers, and Eddelbuettel (2018)</span>).</p>
<p>Most commonly used unconditional performance scores <span class="math inline">\(S(\mathcal{D})\)</span> can be seen as expected values over conditional scores <span class="math inline">\(S(\mathcal{D}|X_1)\)</span> by <span class="math inline">\(S(\mathcal{D}) = \boldsymbol{E}\big[S(\mathcal{D}|X_1)\big]\)</span> in a similar way as power and expected sample size. Any such ‘integral score’ can be computed quickly and reliably in <strong>adoptr</strong> via the choice of pivots outlined above. The correctness of numerically integrated scores is checked in the <strong>adoptr</strong> validation report by comparing the numerical integrals to simulated results.</p>
<p>Note that we tacitly relaxed all sample sizes to be real numbers in the above argument while they are in fact restricted to positive integers. Integer-valued <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> would, however, lead to an NP-hard mixed-integer problem. In our experiments, we found that merely rounding both <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> after the optimization works fine. The extensive validation suite (cf. Section 7) evaluates by numerical integration and simulation whether the included constraints are fulfilled for optimal designs with rounded sample sizes. Up to now, neither the constraints were violated nor an efficiency loss with respect to the underlying objective function was observed. In theory, one could re-adjust the decision boundaries for these rounded sample sizes, but we failed to see any practical benefit from this, even for small trials where the rounding error is largest (data not shown).</p>
</div>
<div id="the-need-for-an-r-package" class="section level1">
<h1 class="hasAnchor">
<a href="#the-need-for-an-r-package" class="anchor"></a>The need for an R package</h1>
<p><span class="citation">Bauer et al. (2015)</span> state that adequate statistical software for adaptive designs ‘is increasingly needed to evaluate the adaptations and to find reasonable strategies’.</p>
<p>Commercial software such as JMP <span class="citation">(SAS Institute Inc., n.d.a)</span> or Minitab <span class="citation">(Minitab, Inc. 2020)</span> allow planning and analyzing a wide range of experimental setups. Amongst others, they provide tools for randomization, stratification, block-building, or D-optimal designs. These general purpose statistical software packages do not, however, allow planning of more specialized multi-stage designs encountered in clinical trials. For group-sequential designs, some planning capabilities are available in the <strong>SAS</strong> procedure <code>seqdesign</code> <span class="citation">(SAS Institute Inc., n.d.b)</span>, PASS <span class="citation">(NCSS 2019)</span>, or ADDPLAN <span class="citation">(ICON plc 2020)</span>. East <span class="citation">(Cytel 2020)</span> also supports design, simulation and analysis of experiments with interim analyses. The East ADAPT and the East SURVADAPT modules support sample size recalculation. Furthermore, there are various open-source <strong>R</strong>-packages for the analysis of multi-stage designs. The package <strong>adaptTest</strong> <span class="citation">(Vandemeulebroecke 2009)</span> implements combination tests for adaptive two-stage designs. <strong>AGSDest</strong> <span class="citation">(Hack, Brannath, and Brueckner 2019)</span> allows estimation and computation of confidence intervals in adaptive group-sequential designs. More detailed overviews on software for adaptive clinical trial designs can be found in <span class="citation">Bauer et al. (2015)</span>, chapter 6, or in <span class="citation">Tymofyeyev (2014)</span>. The choice of software for optimally designing two- or multi-stage designs, however, is much more limited. Current <strong>R</strong>-packages concerned with optimal clinical trial designs are <strong>OptGS</strong> (<span class="citation">Wason and Burkardt (2015)</span>, <span class="citation">Wason (2015)</span>) and <strong>rpact</strong> <span class="citation">(Wassmer and Pahlke 2019)</span>. These are, however, exclusively focused on group-sequential designs and lack the ability to specify custom objective functions and constraints.</p>
<p>The lack of flexibility in formulating the objective function and constraints might lead to off-the-shelf solutions not entirely reflecting the needs of a particular trial consequentially resulting in inefficient designs. The <strong>R</strong>-package <strong>adoptr</strong> aims at providing a simple and interactive yet flexible interface for addressing a range of optimization problems encountered with two-stage one- or two-arm clinical trials. In particular, <strong>adoptr</strong> allows to model <em>a priori</em> uncertainty over <span class="math inline">\(\theta\)</span> via prior distributions and thus supports optimization under uncertainty (cf. Section <a href="#sec:uncertainty">6.2</a>). <strong>adoptr</strong> also supports the combination of conditional (on <span class="math inline">\(X_1\)</span>) and unconditional scores and constraints to address concerns such as type-one-error-rate control (unconditional score) and, e.g., a minimal conditional power (conditional score) simultaneously (cf. Section <a href="#sec:utility_maximization">6.3</a>). To facilitate the adoption of these advanced trial designs in the clinical trials community, <strong>adoptr</strong> also features an extensive test and validation suite (cf. Section 7).</p>
<p>In the following, we outline the key design principles for <strong>adoptr</strong>.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Interactivity:</strong> A major advantage of the <strong>R</strong>-programming language is its powerful metaprogramming capabilities and flexible class system. With a combination of non-standard evaluation and S4 classes, we hope to achieve a structured and modular way of expressing optimization problems in clinical trials that integrates nicely with an interactive workflow. We feel that a step-wise problem formulation via the creation of modular intermediate objects, which can be explored and modified separately, encourages exploration of different options.</li>
<li>
<strong>Reliability:</strong> A crux in open-source software development for clinical trials is achieving demonstrable validation. Potential users need to be convinced of the software quality and need to be able to comply with their respective validation requirements which often require the ability to produce a validation report. This burden typically results in innovative software not being used at all - simply because the validation effort cannot be stemmed. We address this issue with an extensive unit test suite and a companion validation report (cf. Section 7).</li>
<li>
<strong>Extensibility:</strong> We do not want to impose a particular choice of scores or constraints or promote a particular notion of optimality for clinical trial designs. In cases where the composition of existing scores is not sufficient, the object-oriented approach of <strong>adoptr</strong> facilitates the definition of custom scores and constraints that seamlessly integrate with the remainder of the package.</li>
</ol>
</div>
<div id="sec:structure" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:structure" class="anchor"></a>Adoptr’s structure</h1>
<p>The package <strong>adoptr</strong> is based on <strong>R</strong>’s S4 class system. This allows to use multiple dispatch on the classes of multiple arguments to a method. In this section, the central components of <strong>adoptr</strong> are described briefly. Figure @ref(fig:class-diagram) gives a structural overview of the main classes in <strong>adoptr</strong>.</p>
<div class="figure">
<img src="structure.png" alt="Overview of the most important classes and methods (in italic) in the R-package adoptr. A subclassing relationship is indicated by a connecting line to the  corresponding super class above it. The most important methods for each class are listed under the respective class name in italic font." width="100%"><p class="caption">
Overview of the most important classes and methods (in italic) in the R-package adoptr. A subclassing relationship is indicated by a connecting line to the corresponding super class above it. The most important methods for each class are listed under the respective class name in italic font.
</p>
</div>
<p>To compute optimal designs, an object of class <code>UnconditionalScore</code> must be defined as objective criterion. <strong>adoptr</strong> distinguishes between <code>ConditionalScore</code>s and <code>UnconditionalScore</code>s (cf. Section <a href="#sec:setting">2</a>). All <code>Score</code>s can be evaluated using the method <code>evaluate</code>. For unconditional scores, this method only requires a <code>Score</code> object and a <code>TwoStageDesign</code> object, for conditional scores (like conditional power), it also requires the interim outcome <span class="math inline">\(x_1\)</span>. Note that any <code>ConditionalScore</code> <span class="math inline">\(S(\mathcal{D}|X_1=x_1)\)</span> can be converted to an <code>UnconditionalScore</code> <span class="math inline">\(S(\mathcal{D}) = \boldsymbol{E}\big[S(\mathcal{D}|X_1)\big]\)</span> using the method <code>expected</code>. The two most widely used conditional scores are pre-implemented as <code>ConditionalPower</code> and <code>ConditionalSampleSize</code>. Their unconditional counterparts are <code>Power</code> and <code>ExpectedSampleSize</code>. Further predefined unconditional scores are <code>MaximumSampleSize</code>, evaluating the maximum sample size, <code>N1</code>, measuring the first-stage sample size, and <code>AverageN2</code>, evaluating the average of the stage-two sample size (improper prior). These scores may be used for regularization if variable stage-two sample sizes or a high stage-one sample size are to be penalized. Users are free to define their own <code>Score</code>s (cf. the vignette ‘Defining New Scores’ <span class="citation">(Kunzmann and Pilz 2020)</span>). Moreover, different <code>Score</code>s can be composed to a single one by the function <code>composite</code> (cf. Section <a href="#sec:utility_maximization">6.3</a>). Both conditional and unconditional scores can also be used to define constraints - the most common case being constraints for power and maximal type one error rate. The function <code>minimize</code> takes an unconditional score as objective and a set of constraints and optimizes the design parameters.</p>
<p>In <strong>adoptr</strong>, different kinds of designs are implemented. The most frequently applied case is a <code>TwoStageDesign</code>, i.e., a design with one interim analysis and a sample size function that varies with the interim test statistic. Another option is the subclass <code>GroupSequentialDesign</code> which restricts the sample size function on the continuation region to a single number, i.e., <span class="math inline">\({n_2(x_1) = n_2\ \forall x_1\in[\,c_1^f, c_1^e\,]}\)</span>. Additionally, <strong>adoptr</strong> supports the computation of optimal <code>OneStageDesign</code>s, i.e., designs without an interim analysis. Technically, one-stage designs are implemented as subclasses of <code>TwoStageDesign</code> since they can be viewed as the limiting case for <span class="math inline">\(n_2\equiv0\)</span> and <span class="math inline">\(c_1^f=c_1^e\)</span>. Hence, all methods that are implemented for <code>TwoStageDesign</code>s also work for <code>GroupSequentialDesign</code>s and <code>OneStageDesign</code>s. Users can chose to keep some elements of a design fixed during optimization using the methods <code>make_fixed</code> (cf. Section <a href="#sec:make_fixed">6.5</a>).</p>
<p>The joint data distribution in <strong>adoptr</strong> consists of two elements. The distribution of the test statistic is specified by an object of class <code>DataDistribution</code>. Currently, the three options <code>Normal</code>, <code>Binomial</code>, and <code>Student</code> are implemented. The logical variable <code>two_armed</code> allows the differentiation between one- and two-armed trials. Furthermore, <strong>adoptr</strong> supports prior distributions on the effect size. These can be <code>PointMassPrior</code>s (cf. Section <a href="#sec:standard_case">6.1</a>) as well as <code>ContinuousPrior</code>s (cf. Section <a href="#sec:uncertainty">6.2</a>).</p>
<p>In the following section, more hands on examples demonstrate the capabilities of <strong>adoptr</strong> and its syntax.</p>
</div>
<div id="sec:Examples" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:Examples" class="anchor"></a>Examples</h1>
<div id="sec:standard_case" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:standard_case" class="anchor"></a>Standard case</h2>
<p>Consider the case of a randomized controlled clinical trial where efficacy is to be demonstrated in terms of superiority of the treatment over placebo with respect to the population mean difference <span class="math inline">\(\theta\)</span> of an outcome. Let the null hypothesis be <span class="math inline">\(\mathcal{H}_0:\theta\leq 0\)</span>. Assume that the maximal type one error rate is to be controlled at a one-sided level <span class="math inline">\(\alpha=2.5\%\)</span> and a minimal power of <span class="math inline">\(90\%\)</span> at a point alternative of <span class="math inline">\(\theta_1=0.3\)</span> is deemed necessary. For simplicity’s sake, we assume <span class="math inline">\(\sigma^2=1\)</span> without loss of generality. The required sample size for a one-stage design with analysis by the one-sided two-sample <span class="math inline">\(t\)</span>-test would then be roughly 235 per group.</p>
<p>Using <strong>adoptr</strong>, the two-stage design minimizing the expected sample size under the alternative hypothesis can be derived for the very same situation. First, the data distribution is specified to be normal. The <code>two_armed</code> parameter allows to switch between single-armed and two-armed trials.</p>
<pre class="text"><code>R&gt; datadist &lt;- Normal(two_armed = TRUE)</code></pre>
<p>In this example, we use simple point priors for both the null and alternative hypotheses. The hypotheses and the corresponding scores (power values) can be specified as:</p>
<pre class="text"><code>R&gt; null        &lt;- PointMassPrior(theta = .0, mass = 1.0)
R&gt; alternative &lt;- PointMassPrior(theta = .3, mass = 1.0)
R&gt; power       &lt;- Power(dist = datadist, prior = alternative)
R&gt; toer        &lt;- Power(dist = datadist, prior = null)
R&gt; mss         &lt;- MaximumSampleSize()</code></pre>
<p>A <code>Power</code> score requires the data distribution and the prior to be specified. For this example, we choose <code>PointMassPrior</code>s with the entire probability mass of <span class="math inline">\(1\)</span> on a single point, the null hypothesis <span class="math inline">\(\theta = 0\)</span> to compute the type one error rate, and the alternative hypothesis <span class="math inline">\(\theta = 0.3\)</span> to compute the power. The objective function is the expected sample size under the alternative.</p>
<pre class="text"><code>R&gt; ess &lt;- ExpectedSampleSize(dist = datadist, prior = alternative)</code></pre>
<p>Since <strong>adoptr</strong> internally relies on the COBYLA implementation of <strong>nloptr</strong>, an initial design is required. A heuristic initial choice is provided by the function <code>get_initial_design</code>. It is based on a fixed design that fulfills constraints on type one error rate and power. The type of the design (two-stage, group-sequential, or one-stage) and the data distribution have to be defined. For the Gaussian quadrature used during optimization, one also has to specify the order of the integration rule, i.e., the number of pivot points between early stopping for futility and early stopping for efficacy. In practice, order 7 turned out to be sufficiently flexible to obtain valid results (data not shown).</p>
<pre class="text"><code>R&gt; initial_design &lt;- get_initial_design(theta = 0.3, alpha = 0.025,
+                                       beta = 0.1, type = "two-stage",
+                                       dist = datadist, order = 7)</code></pre>
<p>It is easy to check that the initial design does not fulfill any of the constraints (minimal power of 90% and maximal type one error rate of 2.5%) with equality by evaluating the respective scores:</p>
<pre class="text"><code>R&gt; evaluate(toer, initial_design)</code></pre>
<pre><code>## [1] 0.0246875</code></pre>
<pre class="text"><code>R&gt; evaluate(power, initial_design)</code></pre>
<pre><code>## [1] 0.8130973</code></pre>
<p>Alternatively, one might also <code>evaluate</code> a constraint object directly via</p>
<pre class="text"><code>R&gt; evaluate(toer  &lt;= .025, initial_design)</code></pre>
<pre><code>## [1] -0.0003125</code></pre>
<pre class="text"><code>R&gt; evaluate(power &gt;= .9, initial_design)</code></pre>
<pre><code>## [1] 0.08690272</code></pre>
<p>All constraint objects are normalized to the form <span class="math inline">\(h(\mathcal{D}) \leq 0\)</span> (unconditional) or <span class="math inline">\(h(\mathcal{D}, x_1) \leq 0\)</span> (conditional on <span class="math inline">\(X_1=x_1\)</span>). Calling <code>evaluate</code> on a constraint object then simply returns the left-hand side of the inequality. The actual optimization is started by invoking <code>minimize</code></p>
<pre class="text"><code>R&gt; opt1 &lt;- minimize(ess, subject_to(power &gt;= 0.9, toer  &lt;= 0.025),
+                   initial_design)</code></pre>
<p>The modular structure of the problem specification is intended to facilitate the inspection or modification of individual components. The call to <code><a href="../reference/minimize.html">minimize()</a></code> is designed to be as close as possible to the mathematical formulation of the optimization problem and returns both the optimized design (<code>opt1$design</code>) as well as the full <strong>nloptr</strong> return value with details on the optimization procedure (<code>opt1$nloptr_return</code>).</p>
<p>A <code>summary</code> method for objects of the class <code>TwoStageDesign</code> is available to quickly evaluate a set of <code>ConditionalScores</code> such as conditional power as well as <code>UnconditionalScores</code> such as power and expected sample size.</p>
<pre class="text"><code>R&gt; cp &lt;- ConditionalPower(dist = datadist, prior = alternative)
R&gt; summary(opt1$design, "Power" = power, "ESS" = ess, "CP" = cp)</code></pre>
<pre><code>## TwoStageDesign: n1 = 120 
## 
          futility |                  continue                 | efficacy
## 
      x1:     0.28 |  0.33  0.54  0.87  1.27  1.68  2.01  2.22 |  2.27
## 
  c2(x1):     +Inf | +2.70 +2.53 +2.23 +1.82 +1.31 +0.74 +0.19 |  -Inf
## 
  n2(x1):        0 |   229   214   188   154   116    79    51 |     0
## 
  CP(x1):     0.00 |  0.69  0.72  0.75  0.79  0.83  0.87  0.91 |  1.00
## 
   Power:      0.899
## 
     ESS:    176.126
## </code></pre>
<p><strong>adoptr</strong> also implements a default plot method for the overall sample size and the stage-two critical value as functions of the first-stage test statistic <span class="math inline">\(x_1\)</span>. The plot method also accepts additional <code>ConditionalScores</code> such as conditional power. Calling the plot method produces several plots with the interim test statistic <span class="math inline">\(x_1\)</span> on the <span class="math inline">\(x\)</span>-axis and the respective function on the <span class="math inline">\(y\)</span>-axis.</p>
<pre class="text"><code>R&gt; plot(opt1$design, `Conditional power` = cp)</code></pre>
<div class="figure">
<img src="adoptr_jss_files/figure-html/standard-case-1.png" alt="Optimal sample size, critical value, and conditional power plotted against the interim test statistic (built-in plot method)." width="100%"><p class="caption">
Optimal sample size, critical value, and conditional power plotted against the interim test statistic (built-in plot method).
</p>
</div>
<p>Note the slightly bent shape of the <span class="math inline">\(c_2(\cdot)\)</span> function (cf. Figure @ref(fig:standard-case), second plot). For two-stage designs based on the inverse-normal combination function, <span class="math inline">\(c_2(\cdot)\)</span> would be linear by definition (cf. Equation @ref(eq:inverse-normal)). Since the optimal shape of <span class="math inline">\(c_2(\cdot)\)</span> is not linear (but almost), inverse-normal combination methods are slightly less efficient (cf. <span class="citation">Pilz et al. (2019)</span> for a more detailed discussion of this issue).</p>
</div>
<div id="sec:uncertainty" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:uncertainty" class="anchor"></a>Optimization under uncertainty</h2>
<p><strong>adoptr</strong> is not limited to point priors but also supports arbitrary continuous prior distributions. Consider the same situation as before but now assume that the prior over the effect size is given by a much more realistic truncated normal distribution with mean <span class="math inline">\(0.3\)</span> and standard deviation <span class="math inline">\(0.1\)</span>, i.e., <span class="math inline">\(\theta\sim\mathcal{N}_{[-1, 1]}(0.3, 0.1^2)\)</span>. The order of integration is set to 25 to obtain precise results.</p>
<pre class="text"><code>R&gt; prior &lt;- ContinuousPrior(
+    pdf     = function(theta) dnorm(theta, mean = .3, sd = .1),
+    support = c(-1, 1),
+    order   = 25)</code></pre>
<p>The objective function is the expected sample size under the prior</p>
<pre class="text"><code>R&gt; ess &lt;- ExpectedSampleSize(dist = datadist, prior = prior)</code></pre>
<p>and we replace power with expected power <span class="math display">\[
\boldsymbol{E} \Big[
\boldsymbol{Pr}_\theta\big[X_2&gt;c_2(X_1)\big] \, \Big| \, \theta \geq 0.1 \Big]
\]</span> which is the expected power given a relevant effect (here we define the minimal relevant effect as <span class="math inline">\(0.1\)</span>). This score can be defined in <strong>adoptr</strong> by first conditioning the prior.</p>
<pre class="text"><code>R&gt; epower &lt;- Power(dist = datadist, prior = condition(prior, c(.1, 1)))</code></pre>
<p>The optimal design under the point prior only achieves an expected power of</p>
<pre class="text"><code>R&gt; evaluate(epower, opt1$design)</code></pre>
<pre><code>## [1] 0.8143914</code></pre>
<p>The optimal design under the truncated normal prior fulfilling the expected power constraint is then given by</p>
<pre class="text"><code>R&gt; opt2 &lt;- minimize(ess, subject_to(epower &gt;= 0.9, toer &lt;= 0.025),
+                   initial_design,
+                   opts = list(algorithm = "NLOPT_LN_COBYLA",
+                               xtol_rel = 1e-5, maxeval = 20000))</code></pre>
<p>Note that the increased complexity of the problem requires a larger maximal number of iterations for the underlying optimization procedure. <strong>adoptr</strong> exposes the <strong>nloptr</strong> options via the argument <code>opts</code>. In cases where the maximal number of iterations is exhausted, a warning is thrown.</p>
<p>The expected sample size under the prior of the obtained optimal design equals 236.2. This shows that an increased uncertainty on <span class="math inline">\(\theta\)</span> requires larger sample sizes to fulfill the expected power constraint since the expected sample size under the continuous prior considered in this section of the optimal design derived under a point alternative (see Section <a href="#sec:standard_case">6.1</a>) is only 176.4.</p>
</div>
<div id="sec:utility_maximization" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:utility_maximization" class="anchor"></a>Utility maximization and composite scores</h2>
<p><strong>adoptr</strong> also supports composite scores. This can be used to derive utility maximizing designs by defining an objective function combining both expected power and expected sample size instead of imposing a hard constraint on expected power. For example, in the above situation one could be interested in a utility-maximizing design. Here, we consider the utility function <span class="math display">\[
u(\mathcal{D}) := 200000\, \boldsymbol{E} \Big[
\boldsymbol{Pr}_\theta\big[X_2&gt;c_2(X_1)\big] \, \Big| \, \theta \geq 0.1 \Big] - \boldsymbol{E}\Big[n(X_1)^2\Big],
\]</span> thus allowing a direct trade-off between power and sample size. Here, the expected  sample size is chosen because the practitioner might prefer flatter sample size curves. This can be achieved with expected squared sample size by penalizing large sample sizes stronger than low sample sizes. Furthermore, there is no longer a strict expected power constraint but the expected power becomes part of the utility function which allows a direct trade-off between the two quantities. This can be interpreted as a pricing mechanism (cf. <span class="citation">Kunzmann and Kieser (2020)</span>): Every additional percent point of expected power has a (positive) value of <span class="math inline">\(\$ 2'000\)</span> while an increase of <span class="math inline">\(\boldsymbol{E}\big[n(X_1)^2\big]\)</span> by <span class="math inline">\(1\)</span> incurs costs of <span class="math inline">\(\$ 1\)</span>. The goal is then to compute the design which is maximizing the overall utility defined by the utility function <span class="math inline">\(u(\mathcal{D})\)</span> (or equivalently minimize costs).</p>
<p>A composite score can be defined via any valid numerical <strong>R</strong> expression of score objects. We start by defining a score for the expected quadratic sample size</p>
<pre class="text"><code>R&gt; `n(X_1)`      &lt;- ConditionalSampleSize()
R&gt; `E[n(X_1)^2]` &lt;- expected(composite({`n(X_1)`^2}),
+                            data_distribution = datadist,
+                            prior = prior)</code></pre>
<p>before minimizing the corresponding negative utility without a hard expected power constraint.</p>
<pre class="text"><code>R&gt; opt3 &lt;- minimize(composite({`E[n(X_1)^2]` - 200000*epower}),
+                   subject_to(toer &lt;= 0.025), initial_design)</code></pre>
<p>The expected power of the design is</p>
<pre class="text"><code>R&gt; evaluate(epower, opt3$design)</code></pre>
<pre><code>## [1] 0.796554</code></pre>
<p>The three optimal designs which have been computed so far are depicted in a joint plot (cf. Figure @ref(fig:comparison)). The design using the continuous prior requires higher sample sizes due to the higher uncertainty about <span class="math inline">\(\theta\)</span>. The utility maximization approach results in similar shapes of <span class="math inline">\(n(\cdot)\)</span> and <span class="math inline">\(c_2(\cdot)\)</span> as the constraint optimization. However, the sample sizes are lower due to the design’s lower power which is only possible by allowing a trade-off between expected power and expected sample size. In particular, the maximal sample size of the utility-based design equals 256 and is distinctly smaller than in the case of a hard power constraint under a point prior (maximal sample size: 352) or a continuous prior (maximal sample size: 527).</p>
<div class="figure">
<img src="adoptr_jss_files/figure-html/comparison-1.png" alt="Comparison of optimal designs under a point prior, a continuous prior, and a utility maximization approach." width="100%"><p class="caption">
Comparison of optimal designs under a point prior, a continuous prior, and a utility maximization approach.
</p>
</div>
</div>
<div id="sec:CP_constraint" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:CP_constraint" class="anchor"></a>Conditional power constraint</h2>
<p><strong>adoptr</strong> also allows the incorporation of hard constraints on conditional scores such as conditional power. Conditional power constraints are intuitively sensible to make sure that a trial which continues to the second stage maintains a high chance of rejecting the null hypothesis at the end. For this example, we return to the case of a point prior on the effect size.</p>
<pre class="text"><code>R&gt; prior &lt;- PointMassPrior(theta = .3, mass = 1.0)
R&gt; ess   &lt;- ExpectedSampleSize(dist = datadist, prior = prior)
R&gt; cp    &lt;- ConditionalPower(dist = datadist, prior = prior)
R&gt; power &lt;- expected(cp, data_distribution = datadist, prior = prior)</code></pre>
<p>Here, power is derived as expected score of the corresponding conditional power. A conditional power constraint is added in exactly the same way as unconditional constraints.</p>
<pre class="text"><code>R&gt; opt4 &lt;- minimize(ess, subject_to(toer &lt;= 0.025, power &gt;= 0.9, cp &gt;= 0.8),
+                   initial_design)</code></pre>
<p>Comparing the optimal design that has been computed here with the same constraints but without a conditional power constraint (cf. beginning of this chapter), the optimal design with the additional constraint requires larger sample sizes in regions where the conditional power would usually be below the given threshold (cf. Figure @ref(fig:cp-constraint), first and third plot). Overall, the additional constraint reduces the feasible solution space and consequently increases the expected sample size (176.6 with conditional power constraint vs. 176.1 without). This example demonstrates, that any additional binding conditional constraints do come at costs for global optimality. Whether or not the loss in unconditional performance is outweighed by more appealing conditional properties must be decided on a case by case basis.</p>
<div class="figure">
<img src="adoptr_jss_files/figure-html/cp-constraint-1.png" alt="Optimal designs with and without conditional power constraint." width="100%"><p class="caption">
Optimal designs with and without conditional power constraint.
</p>
</div>
</div>
<div id="sec:make_fixed" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:make_fixed" class="anchor"></a>Keeping design parameters fixed</h2>
<p>In clinical practice, non-statistical considerations may impose direct constraints on design parameters. For instance, a sponsor might be subject to logistical constraints that render it necessary to design a trial with a specific stage one sample size. Returning to the standard case discussed in Section <a href="#sec:standard_case">6.1</a>, assume that a stage-one per-group sample size of exactly 80 individuals per group is required (<span class="math inline">\(n_1 = 80\)</span>) instead of the optimal value of <span class="math inline">\(n_1^* = 120\)</span>. Furthermore, assume that the sponsor wants to stop early for futility if and only if there is a negative effect size at the interim analysis, i.e., <span class="math inline">\(c_1^f = 0\)</span>. <strong>adoptr</strong> supports such considerations by allowing to fix specific values of a design:</p>
<pre class="text"><code>R&gt; initial_design@n1  &lt;- 80
R&gt; initial_design@c1f &lt;- 0
R&gt; initial_design     &lt;- make_fixed(initial_design, n1, c1f)</code></pre>
<p>Any ‘fixed’ parameter will be kept constant during optimization. Note that it is also possible to ‘un-fix’ parameters again using the <code>make_tunable</code> function.</p>
<pre class="text"><code>R&gt; opt5 &lt;- minimize(ess, subject_to(toer &lt;= 0.025, power &gt;= 0.9),
+                   initial_design)</code></pre>
<p>Figure @ref(fig:tunable) visually compares the original design with the new, more restricted design. The designs are qualitatively similar, but fixing <span class="math inline">\(n_1\)</span> and <span class="math inline">\(c_1^f\)</span> does come at the price of slightly increased expected sample size (187.7 compared to 176.1 in the less restricted case).</p>
<div class="figure">
<img src="adoptr_jss_files/figure-html/tunable-1.png" alt="Comparison of fully optimal design and optimal design with fixed first-stage sample size." width="100%"><p class="caption">
Comparison of fully optimal design and optimal design with fixed first-stage sample size.
</p>
</div>
</div>
</div>
<div id="sec:validation" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:validation" class="anchor"></a>Validation concept</h1>
<p>The conduct and analysis of clinical trials is a highly regulated process. An essential requirement being put forward in Title 21 CRF (code of federal regulations) Part 11 is the need to validate any software used to work with or produce records <span class="citation">(US Food and Drug Administration and others 2003)</span>. The exact scope of regulations such as CRF 11 is sometimes difficult to assess, and it is not always clear which regulations apply to <strong>R</strong> packages used in a production environment <span class="citation">(The R Foundation for Statistical Computing 2013)</span>. Irrespective of the applicability of the CRF 11 to <strong>adoptr</strong>, the design of a clinical trial is undoubtedly crucial and package authors should provide extensive evidence of the correctness of the package functionality. Additionally, this evidence should be easily accessible and human-readable. The latter requirement is a consequence of the fact that, again following CRF 11 and the remarks in <span class="citation">The R Foundation for Statistical Computing (2013)</span>, a ‘validated <strong>R</strong>-package’ does not exist since the validation process must always be implemented by the responsible user.</p>
<p>To facilitate the process of validation as much as possible, <strong>adoptr</strong> implements the following measures:</p>
<ol style="list-style-type: decimal">
<li>
<em>Open-source development:</em> The entire development of <strong>adoptr</strong> is organized around a public GitHub.com repository (<a href="https://github.com/kkmann/adoptr" class="uri">https://github.com/kkmann/adoptr</a>). Anybody can freely download the source code, browse the development history, raise issues, or contribute to the code base by opening pull requests.</li>
<li>
<em>CRAN releases:</em> Regular CRAN <span class="citation">(CRAN 2020)</span> releases of updated versions maximize visibility and add an additional layer of testing and quality control. New features can be implemented and tested in the (public) development version on GitHub before pushing new releases to CRAN.</li>
<li>
<em>Unit testing:</em> <strong>adoptr</strong> implements an extensive test suite using the package <strong>testthat</strong> (<span class="citation">Wickham, R Studio, and R Core Team (2018)</span>, <span class="citation">Wickham (2011)</span>) which allows spotting new errors early during development and localizing them quickly. Together with continuous integration (cf. below), this helps to improve quality and speeds up the development process.</li>
<li>
<em>Continuous Integration / Continuous Deployment:</em> <strong>adoptr</strong> makes extensive use of the continuous integration and deployment services GitHub Actions <span class="citation">(GitHub.com 2021)</span>. Each new commit on the public GitHub.com repository is immediately run through the automated testing pipeline. Merges to the master branch are only possible after tests were passed successfully and a contributor reviewed and approved the changes (‘branch protection system’). Continuous deployment allows automatically updating code-coverage statistics (cf. below) and up-to-date online documentation (cf. below).</li>
<li>
<em>Coverage analyses:</em> To document the extent to which the test suite covers the package code, <strong>adoptr</strong> relies on the codecov <span class="citation">(Codecov LLC 2020)</span> online service in conjunction with the <strong>covr</strong> package <span class="citation">(Hester 2019)</span>. It provides statistics on the proportion of lines visited at least once during testing (currently 100%) and enables easy online publication of the results.</li>
<li>
<em>Online documentation:</em> Beyond the standard documentation generated using <strong>roxygen2</strong> <span class="citation">(Hadley Wickham et al. 2019)</span>, we also make use of the <strong>pkgdown</strong> <span class="citation">(H. Wickham and Hesselberth 2019)</span> package and the free GitHub pages service to publish a static <strong>html</strong> version of the documentation online at <a href="https://kkmann.github.io/adoptr/" class="uri">https://kkmann.github.io/adoptr/</a>. This includes both the function reference and the vignettes in a consistent and easily accessible format. The online documentation experience is further improved by the integration of a full-text docsearch engine (<a href="https://www.algolia.com/ref/docsearch" class="uri">https://www.algolia.com/ref/docsearch</a>).</li>
<li>
<em>Extended validation report:</em> There are limits to what can be done in the standard unit testing framework within a package itself (cf. <a href="https://cran.r-project.org/web/packages/policies.html" class="uri">https://cran.r-project.org/web/packages/policies.html</a>). Long-running test suites also hinder active development with a strict continuous integration and continuous deployment (CI/CD) workflow since changes to the master branch require passing the automated tests. We, therefore, decided to restrict the internal unit tests to a bare minimum with a clear focus on coverage and technical integrity of the package. To demonstrate correctness of our results over a larger set of examples and in comparison with existing packages such as <strong>rpact</strong>, we implemented an external ‘validation report’ (sources: <a href="https://github.com/kkmann/adoptr-validation-report" class="uri">https://github.com/kkmann/adoptr-validation-report</a>, current report: <a href="https://kkmann.github.io/adoptr-validation-report/" class="uri">https://kkmann.github.io/adoptr-validation-report/</a>) using the <strong>bookdown</strong> (<span class="citation">Xie (2019)</span>, <span class="citation">Xie (2016)</span>) package. The report itself again uses CI/CD and daily rebuilds to automatically deploy the report corresponding to the most current CRAN-hosted version of the package. Within the report, we still use <strong>testthat</strong> to conduct formal tests. In case any of these tests fails, the build of the report will fail, the maintainers will get notified, and the status indicator in the repository changes.</li>
</ol>
<p>Validating the software employed may well be as much work as developing it in the first place. The opaque requirements and the lack of adequate tools to automate validation tasks are a major hurdle for academic developers to address validation issues. The additional work, however, is worth it since it not only improves quality but also facilitates collaboration and makes it easier to promote packages for real-world use.</p>
</div>
<div id="future-work" class="section level1">
<h1 class="hasAnchor">
<a href="#future-work" class="anchor"></a>Future work</h1>
<p>The main motivation of implementing <strong>adoptr</strong> in <strong>R</strong> is the fact that this is by far the most common programming language used by the target audience. Note, however, that using <strong>R</strong> for generic nonlinear constraint optimization problems leads to a performance bottleneck since there is currently no stable and efficient way of obtaining gradient information for generic, user-defined functions. Since one of the design principles of <strong>adoptr</strong> is extensibility, the ability to support custom objective functions is central. In <strong>R</strong>, this implies that one has to resort to either a finite differences approximation of first- and second-order derivatives or to a completely gradient-free optimizer such as COBYLA. In our experiments, we found that COBYLA was far more stable than a finite-differences augmented Lagrangian method (data not shown). Still, for some problems, convergence using COBYLA is rather slow. An interesting alternative to <strong>R</strong> and <strong>nloptr</strong> would therefore be <strong>Julia</strong> <span class="citation">(Bezanson et al. 2017)</span> and the <strong>JuMP</strong> framework for numerical programming <span class="citation">(Lubin and Dunning 2015)</span>. This framework allows interfacing generic nonlinear solvers via a common interface and, leveraging <strong>Julia</strong>’s excellent automatic-differentiation capabilities, is able to provide fast and precise (second-order) gradient information for user-defined objective functions.</p>
</div>
<div id="acknowledgments" class="section level1">
<h1 class="hasAnchor">
<a href="#acknowledgments" class="anchor"></a>Acknowledgments</h1>
<p>The first two authors contributed equally to this manuscript.</p>
<p>This work was partly supported by the Deutsche Forschungsgemeinschaft under Grant number KI 708/4-1.</p>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>

<div id="refs" class="references">
<div id="ref-Bauer2015">
<p>Bauer, P., F. Bretz, V. Dragalin, F. König, and G. Wassmer. 2015. “Twenty-Five Years of Confirmatory Adaptive Designs: Opportunities and Pitfalls.” <em>Statistics in Medicine</em> 35 (3): 325–47. <a href="https://doi.org/10.1002/sim.6472">https://doi.org/10.1002/sim.6472</a>.</p>
</div>
<div id="ref-BK1994">
<p>Bauer, P., and K. Köhne. 1994. “Evaluation of Experiments with Adaptive Interim Analyses.” <em>Biometrics</em> 50 (4): 1029–41.</p>
</div>
<div id="ref-Julia">
<p>Bezanson, J., A. Edelman, S. Karpinski, and V. Shah. 2017. “Julia: A Fresh Approach to Numerical Computing.” <em>SIAM Review</em> 59 (1): 65–98. <a href="https://doi.org/10.1137/141000671">https://doi.org/10.1137/141000671</a>.</p>
</div>
<div id="ref-codecov">
<p>Codecov LLC. 2020. <em>Codecov</em>. <a href="https://about.codecov.io/">https://about.codecov.io/</a>.</p>
</div>
<div id="ref-EMA">
<p>Committee for Medicinal Products for Human Use and others. 2007. “Reflection Paper on Methodological Issues in Confirmatory Clinical Trials Planned with an Adaptive Design.” <em>London: EMEA</em>. <a href="https://www.ema.europa.eu/en/methodological-issues-confirmatory-clinical-trials-planned-adaptive-design">https://www.ema.europa.eu/en/methodological-issues-confirmatory-clinical-trials-planned-adaptive-design</a>.</p>
</div>
<div id="ref-cran">
<p>CRAN. 2020. <em>Comprehensive R Archive Network</em>. <a href="https://cran.r-project.org/">https://cran.r-project.org/</a>.</p>
</div>
<div id="ref-east2020">
<p>Cytel. 2020. <em>East<sup></sup> 6</em>. <a href="https://www.cytel.com/software/east/">https://www.cytel.com/software/east/</a>.</p>
</div>
<div id="ref-Englert2013">
<p>Englert, S., and M. Kieser. 2013. “Optimal Adaptive Two-Stage Designs for Phase Ii Cancer Clinical Trials.” <em>Biometrical Journal</em> 55: 955–68. <a href="https://doi.org/10.1002/bimj.201200220">https://doi.org/10.1002/bimj.201200220</a>.</p>
</div>
<div id="ref-Spline">
<p>Fritsch, F. N., and R. E. Carlson. 1980. “Monotone Piecewise Cubic Interpolation.” <em>SIAM Journal on Numerical Analysis</em> 17 (2): 238–46. <a href="https://doi.org/10.1137/0717021">https://doi.org/10.1137/0717021</a>.</p>
</div>
<div id="ref-ghactions">
<p>GitHub.com. 2021. <em>GitHub Actions</em>. <a href="https://docs.github.com/en/actions/">https://docs.github.com/en/actions/</a>.</p>
</div>
<div id="ref-AGSDest">
<p>Hack, N., W. Brannath, and M. Brueckner. 2019. <em>AGSDest: Estimation in Adaptive Group Sequential Trials</em>. <a href="https://CRAN.R-project.org/package=AGSDest">https://CRAN.R-project.org/package=AGSDest</a>.</p>
</div>
<div id="ref-HO1985">
<p>Hedges, L., and I. Olkin. 1985. <em>Statistical Methods in Meta-Analysis</em>. Academic Press.</p>
</div>
<div id="ref-covr">
<p>Hester, Jim. 2019. <em>Covr: Test Coverage for Packages</em>. <a href="https://CRAN.R-project.org/package=covr">https://CRAN.R-project.org/package=covr</a>.</p>
</div>
<div id="ref-addplan2020">
<p>ICON plc. 2020. <em>ADDPLAN<sup></sup></em>. <a href="https://www.iconplc.com/innovation/addplan/">https://www.iconplc.com/innovation/addplan/</a>.</p>
</div>
<div id="ref-JT2015">
<p>Jennison, C., and B. W. Turnbull. 2015. “Adaptive Sample Size Modification in Clinical Trials: Start Small Then Ask for More?” <em>Statistics in Medicine</em> 34 (29): 3793–3810. <a href="https://doi.org/10.1002/sim.6575">https://doi.org/10.1002/sim.6575</a>.</p>
</div>
<div id="ref-nlopt">
<p>Johnson, S. G. 2018. <em>The Nlopt Nonlinear-Optimization Package</em>. <a href="https://nlopt.readthedocs.io/en/latest/">https://nlopt.readthedocs.io/en/latest/</a>.</p>
</div>
<div id="ref-Kunzmann2016">
<p>Kunzmann, K., and M. Kieser. 2016. “Optimal Adaptive Two-Stage Designs for Single-Arm Trial with Binary Endpoint.” <em>arXiv</em>, 1605.00249.</p>
</div>
<div id="ref-Kunzmann2019">
<p>———. 2020. “Optimal Adaptive Single-Arm Phase Ii Trials Under Quantified Uncertainty.” <em>Journal of Biopharmaceutical Statistics</em> 30 (1): 89–103. <a href="https://doi.org/10.1080/10543406.2019.1609016">https://doi.org/10.1080/10543406.2019.1609016</a>.</p>
</div>
<div id="ref-new-scores">
<p>Kunzmann, K., and M. Pilz. 2020. <em>Defining New Scores</em>. <a href="https://rdrr.io/cran/adoptr/f/vignettes/defining-new-scores.Rmd">https://rdrr.io/cran/adoptr/f/vignettes/defining-new-scores.Rmd</a>.</p>
</div>
<div id="ref-LW1999">
<p>Lehmacher, W., and G. Wassmer. 1999. “Adaptive Sample Size Calculations in Group Sequential Trials.” <em>Biometrics</em> 55 (4): 1286–90. <a href="https://doi.org/10.1111/j.0006-341X.1999.01286.x">https://doi.org/10.1111/j.0006-341X.1999.01286.x</a>.</p>
</div>
<div id="ref-JuMP">
<p>Lubin, M., and I. Dunning. 2015. “Computing in Operations Research Using Julia.” <em>INFORMS Journal on Computing</em> 27 (2): 238–48. <a href="https://doi.org/10.1287/ijoc.2014.0623">https://doi.org/10.1287/ijoc.2014.0623</a>.</p>
</div>
<div id="ref-MP2011">
<p>Mehta, C. R., and S. J. Pocock. 2011. “Adaptive Increase in Sample Size When Interim Results Are Promising: A Practical Guide with Examples.” <em>Statistics in Medicine</em> 30 (28): 3267–84. <a href="https://doi.org/10.1002/sim.4102">https://doi.org/10.1002/sim.4102</a>.</p>
</div>
<div id="ref-minitab2020">
<p>Minitab, Inc. 2020. <em>Minitab 19 Statistical Software <sup></sup></em>. <a href="https://www.minitab.com">https://www.minitab.com</a>.</p>
</div>
<div id="ref-pass2019">
<p>NCSS. 2019. <em>PASS Sample Size 2019 <sup></sup></em>. <a href="https://www.ncss.com/software/pass/">https://www.ncss.com/software/pass/</a>.</p>
</div>
<div id="ref-OBF">
<p>O’Brien, P. C., and T. R. Fleming. 1979. “A Multiple Testing Procedure for Clinical Trials.” <em>Biometrics</em> 35 (3): 549–56.</p>
</div>
<div id="ref-variational">
<p>Pilz, M., K. Kunzmann, C. Herrmann, G. Rauch, and M. Kieser. 2019. “A Variational Approach to Optimal Two-Stage Designs.” <em>Statistics in Medicine</em> 38 (21): 4159–71. <a href="https://doi.org/10.1002/sim.8291">https://doi.org/10.1002/sim.8291</a>.</p>
</div>
<div id="ref-Pocock">
<p>Pocock, S. J. 1977. “Group Sequential Methods in the Design and Analysis of Clinical Trials.” <em>Biometrika</em> 64 (2): 191–99. <a href="https://doi.org/10.1093/biomet/64.2.191">https://doi.org/10.1093/biomet/64.2.191</a>.</p>
</div>
<div id="ref-COBYLA">
<p>Powell, M. J. D. 1994. “A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation.” In <em>Advances in Optimization and Numerical Analysis</em>, 51–67. Dordrecht: Springer-Verlag Netherlands. <a href="https://doi.org/10.1007/978-94-015-8330-5_4">https://doi.org/10.1007/978-94-015-8330-5_4</a>.</p>
</div>
<div id="ref-R">
<p>R Core Team. 2019. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: The R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-jmp2020">
<p>SAS Institute Inc., NC, Cary. n.d.a. <em>JMP Clinical <sup></sup></em>.</p>
</div>
<div id="ref-sas2020">
<p>———. n.d.b. <em>SAS<sup></sup></em>.</p>
</div>
<div id="ref-r-fda">
<p>The R Foundation for Statistical Computing. 2013. “R: Regulatory Compliance and Validation Issues - a Guidance Document for the Use of R in Regulated Clinical Trial Environments.” <a href="https://www.r-project.org/doc/R-FDA.pdf">https://www.r-project.org/doc/R-FDA.pdf</a>.</p>
</div>
<div id="ref-Tymofyeyev2014">
<p>Tymofyeyev, Y. 2014. “A Review of Available Software and Capabilities for Adaptive Designs.” In <em>Practical Considerations for Adaptive Trial Design and Implementation</em>, 139–55. Springer-Verlag New York. <a href="https://doi.org/10.1007/978-1-4939-1100-4_8">https://doi.org/10.1007/978-1-4939-1100-4_8</a>.</p>
</div>
<div id="ref-crf11">
<p>US Food and Drug Administration and others. 2003. “Guidance for Industry Part 11, Electronic Records; Electronic Signatures — Scope and Application.” <em>US Food Drug Admin, Rockville</em>. <a href="https://www.fda.gov/media/75414/download">https://www.fda.gov/media/75414/download</a>.</p>
</div>
<div id="ref-FDA">
<p>US Food and Drug Administration, and others. 2019. <em>Adaptive Designs for Clinical Trials of Drugs and Biologics - Guidance for Industry</em>. <a href="https://www.fda.gov/media/78495/download">https://www.fda.gov/media/78495/download</a>.</p>
</div>
<div id="ref-adaptTest">
<p>Vandemeulebroecke, M. 2009. <em>AdaptTest: Adaptive Two-Stage Tests</em>. <a href="https://CRAN.R-project.org/package=adaptTest">https://CRAN.R-project.org/package=adaptTest</a>.</p>
</div>
<div id="ref-OptGS_pub">
<p>Wason, J. M. S. 2015. “OptGS: An R Package for Finding Near-Optimal Group-Sequential Designs.” <em>Journal of Statistical Software</em> 66 (2): 1–13. <a href="https://doi.org/10.18637/jss.v066.i02">https://doi.org/10.18637/jss.v066.i02</a>.</p>
</div>
<div id="ref-OptGS">
<p>Wason, J. M. S., and J. Burkardt. 2015. <em>OptGS: Near-Optimal and Balanced Group-Sequential Designs for Clinical Trials with Continuous Outcomes</em>. <a href="https://CRAN.R-project.org/package=OptGS">https://CRAN.R-project.org/package=OptGS</a>.</p>
</div>
<div id="ref-Wassmer2016">
<p>Wassmer, G., and W. Brannath. 2016. <em>Group Sequential and Confirmatory Adaptive Designs in Clinical Trials</em>. Springer Series in Pharmaceutical Statistics -. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-32562-0">https://doi.org/10.1007/978-3-319-32562-0</a>.</p>
</div>
<div id="ref-rpact">
<p>Wassmer, G., and F. Pahlke. 2019. <em>Rpact: Confirmatory Adaptive Clinical Trial Design and Analysis</em>. <a href="https://CRAN.R-project.org/package=rpact">https://CRAN.R-project.org/package=rpact</a>.</p>
</div>
<div id="ref-testthat_pub">
<p>Wickham, H. 2011. “Testthat: Get Started with Testing.” <em>The R Journal</em> 3: 5–10. <a href="https://doi.org/10.32614/RJ-2011-002">https://doi.org/10.32614/RJ-2011-002</a>.</p>
</div>
<div id="ref-roxygen">
<p>Wickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster. 2019. <em>Roxygen2: In-Line Documentation for R</em>. <a href="https://CRAN.R-project.org/package=roxygen2">https://CRAN.R-project.org/package=roxygen2</a>.</p>
</div>
<div id="ref-pkgdown">
<p>Wickham, H., and J. Hesselberth. 2019. <em>Pkgdown: Make Static Html Documentation for a Package</em>. <a href="https://CRAN.R-project.org/package=pkgdown">https://CRAN.R-project.org/package=pkgdown</a>.</p>
</div>
<div id="ref-testthat">
<p>Wickham, H., R Studio, and R Core Team. 2018. <em>Testthat: Unit Testing for R</em>. <a href="https://CRAN.R-project.org/package=testthat">https://CRAN.R-project.org/package=testthat</a>.</p>
</div>
<div id="ref-bookdown_pub">
<p>Xie, Y. 2016. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781315204963">https://doi.org/10.1201/9781315204963</a>.</p>
</div>
<div id="ref-bookdown">
<p>———. 2019. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. <a href="https://github.com/rstudio/bookdown">https://github.com/rstudio/bookdown</a>.</p>
</div>
<div id="ref-nloptr">
<p>Ypma, J., H. W. Borchers, and D. Eddelbuettel. 2018. <em>Nloptr: R Interface to Nlopt</em>. <a href="https://CRAN.R-project.org/package=nloptr">https://CRAN.R-project.org/package=nloptr</a>.</p>
</div>
<div id="ref-Zaykin2011">
<p>Zaykin, D. V. 2011. “Optimally Weighted Z-Test Is a Powerful Method for Combining Probabilities in Meta-Analysis.” <em>Journal of Evolutionary Biology</em> 24 (8): 1836–41. <a href="https://doi.org/10.1111/j.1420-9101.2011.02297.x">https://doi.org/10.1111/j.1420-9101.2011.02297.x</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Kevin Kunzmann, Maximilian Pilz.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: 'f5b8253ef131d8de31a2e0e075678ba8',
    indexName: 'adoptr',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
